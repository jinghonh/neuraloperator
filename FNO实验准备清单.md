# FNO (Fourier Neural Operator) å®éªŒå‡†å¤‡æ¸…å•

æœ¬æ–‡æ¡£æä¾›äº†ä½¿ç”¨ FNO è¿›è¡Œå®éªŒæ‰€éœ€çš„å®Œæ•´å‡†å¤‡æ¸…å•ï¼Œæ¶µç›–ä»ç¯å¢ƒé…ç½®åˆ°å®éªŒæ‰§è¡Œçš„å„ä¸ªç¯èŠ‚ã€‚

---

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#1-ç¯å¢ƒå‡†å¤‡)
2. [ç¡¬ä»¶è¦æ±‚](#2-ç¡¬ä»¶è¦æ±‚)
3. [é¡¹ç›®å®‰è£…](#3-é¡¹ç›®å®‰è£…)
4. [æ•°æ®å‡†å¤‡](#4-æ•°æ®å‡†å¤‡)
5. [æ¨¡å‹é…ç½®](#5-æ¨¡å‹é…ç½®)
6. [è®­ç»ƒè®¾ç½®](#6-è®­ç»ƒè®¾ç½®)
7. [è¯„ä¼°ä¸å¯è§†åŒ–](#7-è¯„ä¼°ä¸å¯è§†åŒ–)
8. [å®éªŒè·Ÿè¸ª](#8-å®éªŒè·Ÿè¸ª)
9. [å¸¸è§é—®é¢˜æ’æŸ¥](#9-å¸¸è§é—®é¢˜æ’æŸ¥)
10. [å¿«é€Ÿå¼€å§‹ç¤ºä¾‹](#10-å¿«é€Ÿå¼€å§‹ç¤ºä¾‹)

---

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 Python ç¯å¢ƒ

- **Python ç‰ˆæœ¬**: Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼ˆæ¨è 3.9+ï¼‰
- **åŒ…ç®¡ç†å™¨**: pip æˆ– conda

### 1.2 è™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨ venv åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # macOS/Linux
# æˆ–
venv\Scripts\activate  # Windows

# æˆ–ä½¿ç”¨ conda
conda create -n neuralop python=3.9
conda activate neuralop
```

### 1.3 æ ¸å¿ƒä¾èµ–åŒ…

æ ¹æ® `requirements.txt`ï¼Œéœ€è¦å®‰è£…ä»¥ä¸‹æ ¸å¿ƒä¾èµ–ï¼š

```bash
# æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio  # PyTorch (æ ¹æ®ä½ çš„CUDAç‰ˆæœ¬é€‰æ‹©)
pip install wandb                        # å®éªŒè·Ÿè¸ª
pip install ruamel.yaml                  # YAML é…ç½®å¤„ç†
pip install configmypy                   # é…ç½®ç±»å‹æ£€æŸ¥
pip install zencfg                       # é…ç½®ç®¡ç†
pip install tensorly                     # å¼ é‡åˆ†è§£
pip install tensorly-torch               # TensorLy PyTorch åç«¯
pip install torch-harmonics              # çƒè°å‡½æ•°
pip install matplotlib                   # å¯è§†åŒ–
pip install opt-einsum                  # ä¼˜åŒ–çš„ einsum
pip install h5py                         # HDF5 æ–‡ä»¶æ”¯æŒ
pip install zarr                         # Zarr æ•°ç»„å­˜å‚¨
```

### 1.4 å®‰è£… NeuralOperator åŒ…

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•å®‰è£…ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
cd /path/to/neuraloperator
pip install -e .[dev]

# æˆ–ä»…å®‰è£…åŸºç¡€åŒ…
pip install -e .
```

---

## 2. ç¡¬ä»¶è¦æ±‚

### 2.1 GPUï¼ˆæ¨èï¼‰

- **CUDA**: æ”¯æŒ CUDA çš„ GPUï¼ˆæ¨è NVIDIA GPUï¼‰
- **æ˜¾å­˜**: 
  - å°å‹å®éªŒï¼ˆ16x16 åˆ†è¾¨ç‡ï¼‰: è‡³å°‘ 4GB
  - ä¸­å‹å®éªŒï¼ˆ32x32 åˆ†è¾¨ç‡ï¼‰: è‡³å°‘ 8GB
  - å¤§å‹å®éªŒï¼ˆ64x64+ åˆ†è¾¨ç‡ï¼‰: è‡³å°‘ 16GB æˆ–æ›´å¤š
- **CUDA ç‰ˆæœ¬**: æ ¹æ® PyTorch ç‰ˆæœ¬é€‰æ‹©ï¼ˆé€šå¸¸ CUDA 11.8 æˆ– 12.1ï¼‰

### 2.2 CPUï¼ˆå¯é€‰ï¼‰

- å¯ä»¥åœ¨ CPU ä¸Šè¿è¡Œå°å‹å®éªŒï¼Œä½†è®­ç»ƒé€Ÿåº¦ä¼šæ˜¾è‘—é™ä½
- æ¨èè‡³å°‘ 8GB RAM

### 2.3 å­˜å‚¨ç©ºé—´

- **æ•°æ®é›†**: æ ¹æ®æ•°æ®é›†å¤§å°ï¼Œé¢„ç•™ 1-10GB ç©ºé—´
- **æ¨¡å‹æ£€æŸ¥ç‚¹**: æ¯ä¸ªæ£€æŸ¥ç‚¹çº¦ 10-100MBï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰
- **æ—¥å¿—å’Œå¯è§†åŒ–**: é¢„ç•™ 1-5GB

---

## 3. é¡¹ç›®å®‰è£…

### 3.1 å…‹éš†æˆ–ç¡®è®¤é¡¹ç›®ç»“æ„

ç¡®ä¿é¡¹ç›®ç›®å½•ç»“æ„æ­£ç¡®ï¼š

```
neuraloperator/
â”œâ”€â”€ neuralop/          # ä¸»åŒ…
â”‚   â”œâ”€â”€ models/        # æ¨¡å‹å®šä¹‰ï¼ˆåŒ…å« fno.pyï¼‰
â”‚   â”œâ”€â”€ data/          # æ•°æ®é›†å’Œæ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ training/      # è®­ç»ƒå·¥å…·
â”‚   â””â”€â”€ layers/        # å±‚å®šä¹‰
â”œâ”€â”€ config/            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ scripts/           # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ examples/          # ç¤ºä¾‹ä»£ç 
â””â”€â”€ requirements.txt   # ä¾èµ–åˆ—è¡¨
```

### 3.2 éªŒè¯å®‰è£…

```bash
# è¿è¡Œæµ‹è¯•ï¼ˆå¯é€‰ï¼‰
pytest neuralop/models/tests/test_fno.py -v

# æˆ–è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
pytest neuralop -v
```

---

## 4. æ•°æ®å‡†å¤‡

### 4.1 ä½¿ç”¨å†…ç½®æ•°æ®é›†ï¼ˆDarcy Flowï¼‰

Darcy Flow æ˜¯æœ€å¸¸ç”¨çš„æµ‹è¯•æ•°æ®é›†ï¼Œå¯ä»¥è‡ªåŠ¨ä¸‹è½½ï¼š

```python
from neuralop.data.datasets import load_darcy_flow_small

# æ•°æ®ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
train_loader, test_loaders, data_processor = load_darcy_flow_small(
    data_root="~/data/darcy/",  # æ•°æ®å­˜å‚¨è·¯å¾„
    n_train=1000,               # è®­ç»ƒæ ·æœ¬æ•°
    batch_size=32,              # æ‰¹æ¬¡å¤§å°
    n_tests=[100, 50],          # æ¯ä¸ªæµ‹è¯•åˆ†è¾¨ç‡çš„æ ·æœ¬æ•°
    test_resolutions=[16, 32],  # æµ‹è¯•åˆ†è¾¨ç‡
    test_batch_sizes=[32, 32],  # æµ‹è¯•æ‰¹æ¬¡å¤§å°
    download=True               # è‡ªåŠ¨ä¸‹è½½
)
```

### 4.2 å‡†å¤‡è‡ªå®šä¹‰æ•°æ®

å¦‚æœä½¿ç”¨è‡ªå®šä¹‰æ•°æ®ï¼Œéœ€è¦ï¼š

1. **æ•°æ®æ ¼å¼**: 
   - è¾“å…¥å’Œè¾“å‡ºéƒ½åº”è¯¥æ˜¯ PyTorch å¼ é‡
   - å½¢çŠ¶: `[batch, channels, height, width]` (2D) æˆ– `[batch, channels, depth, height, width]` (3D)

2. **æ•°æ®åŠ è½½å™¨**:
   ```python
   from torch.utils.data import DataLoader, Dataset
   
   class YourDataset(Dataset):
       def __init__(self, ...):
           # åˆå§‹åŒ–
           pass
       
       def __getitem__(self, idx):
           return {"x": input_tensor, "y": output_tensor}
   
   train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
   ```

3. **æ•°æ®é¢„å¤„ç†**:
   - ä½¿ç”¨ `neuralop.data.transforms.DataProcessor` è¿›è¡Œå½’ä¸€åŒ–
   - æˆ–æ‰‹åŠ¨å®ç°é¢„å¤„ç†ç®¡é“

### 4.3 æ•°æ®è·¯å¾„é…ç½®

åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®æ•°æ®è·¯å¾„ï¼š

```python
# config/darcy_config.py
class DarcyDatasetConfig(ConfigBase):
    folder: str = "~/data/darcy/"  # ä¿®æ”¹ä¸ºä½ çš„æ•°æ®è·¯å¾„
    batch_size: int = 8
    n_train: int = 1000
    train_resolution: int = 16
    # ...
```

---

## 5. æ¨¡å‹é…ç½®

### 5.1 åŸºæœ¬ FNO é…ç½®

FNO çš„å…³é”®å‚æ•°ï¼š

```python
from neuralop.models import FNO

model = FNO(
    n_modes=(16, 16),          # Fourier æ¨¡å¼æ•°ï¼ˆæ¯ä¸ªç»´åº¦ï¼‰
    in_channels=1,              # è¾“å…¥é€šé“æ•°
    out_channels=1,             # è¾“å‡ºé€šé“æ•°
    hidden_channels=64,          # éšè—å±‚é€šé“æ•°ï¼ˆæ¨¡å‹å®½åº¦ï¼‰
    n_layers=4,                 # FNO å±‚æ•°
    lifting_channel_ratio=2,     # Lifting å±‚é€šé“æ¯”ä¾‹
    projection_channel_ratio=2, # Projection å±‚é€šé“æ¯”ä¾‹
)
```

### 5.2 é¢„å®šä¹‰é…ç½®

é¡¹ç›®æä¾›äº†å¤šä¸ªé¢„å®šä¹‰é…ç½®ï¼ˆåœ¨ `config/models.py` ä¸­ï¼‰ï¼š

- **FNO_Small2d**: å°å‹ 2D FNO
  - `n_modes=[16, 16]`, `hidden_channels=24`
- **FNO_Medium2d**: ä¸­å‹ 2D FNO
  - `n_modes=[64, 64]`, `hidden_channels=64`
- **FNO_Large2d**: å¤§å‹ 2D FNO
  - `n_modes=[64, 64]`, `hidden_channels=128`
- **FNO_Medium3d**: ä¸­å‹ 3D FNO
  - `n_modes=[32, 32, 32]`, `hidden_channels=64`

### 5.3 é«˜çº§é…ç½®é€‰é¡¹

```python
model = FNO(
    # ... åŸºæœ¬å‚æ•° ...
    
    # å½’ä¸€åŒ–
    norm="group_norm",  # æˆ– "instance_norm", "ada_in", None
    
    # è·³è¿‡è¿æ¥
    fno_skip="linear",  # æˆ– "identity", "soft-gating", None
    channel_mlp_skip="soft-gating",
    
    # åŸŸå¡«å……ï¼ˆç”¨äºå¤„ç†è¾¹ç•Œï¼‰
    domain_padding=0.1,  # å¡«å……ç™¾åˆ†æ¯”
    
    # ç²¾åº¦æ§åˆ¶
    fno_block_precision="full",  # æˆ– "half", "mixed"
    stabilizer="tanh",  # ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ
    
    # å¼ é‡åˆ†è§£ï¼ˆå‡å°‘å‚æ•°é‡ï¼‰
    factorization="Tucker",  # æˆ– "CP", "TT", None
    rank=0.1,  # åˆ†è§£ç§©ï¼ˆ0.1 è¡¨ç¤ºçº¦ 10% çš„å‚æ•°é‡ï¼‰
    
    # ä½ç½®ç¼–ç 
    positional_embedding="grid",  # æˆ– None, GridEmbeddingND
)
```

### 5.4 ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
# ä½¿ç”¨ zencfg é…ç½®ç³»ç»Ÿ
from config.darcy_config import Default
from neuralop import get_model

config = Default()
config.model = FNO_Small2d()  # æˆ–è‡ªå®šä¹‰é…ç½®
model = get_model(config)
```

---

## 6. è®­ç»ƒè®¾ç½®

### 6.1 ä¼˜åŒ–å™¨é…ç½®

```python
from neuralop.training import AdamW

optimizer = AdamW(
    model.parameters(),
    lr=5e-3,              # å­¦ä¹ ç‡ï¼ˆé€šå¸¸ 1e-3 åˆ° 1e-2ï¼‰
    weight_decay=1e-4      # æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰
)
```

### 6.2 å­¦ä¹ ç‡è°ƒåº¦å™¨

```python
import torch.optim.lr_scheduler as lr_scheduler

# StepLR: æ¯ N ä¸ª epoch é™ä½å­¦ä¹ ç‡
scheduler = lr_scheduler.StepLR(
    optimizer,
    step_size=60,          # æ¯ 60 ä¸ª epoch
    gamma=0.5             # å­¦ä¹ ç‡ä¹˜ä»¥ 0.5
)

# CosineAnnealingLR: ä½™å¼¦é€€ç«
scheduler = lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=100             # å‘¨æœŸé•¿åº¦
)

# ReduceLROnPlateau: åŸºäºéªŒè¯æŸå¤±
scheduler = lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.5,
    patience=10
)
```

### 6.3 æŸå¤±å‡½æ•°

```python
from neuralop import LpLoss, H1Loss

# L2 æŸå¤±ï¼ˆä»…å‡½æ•°å€¼ï¼‰
l2loss = LpLoss(d=2, p=2)  # d: ç»´åº¦, p: Lp èŒƒæ•°çš„ p

# H1 æŸå¤±ï¼ˆå‡½æ•°å€¼ + æ¢¯åº¦ï¼‰
h1loss = H1Loss(d=2)

# è®­ç»ƒæ—¶é€šå¸¸ä½¿ç”¨ H1Lossï¼ˆå¯¹ PDE é—®é¢˜æ›´åˆé€‚ï¼‰
train_loss = h1loss
eval_losses = {"h1": h1loss, "l2": l2loss}
```

### 6.4 è®­ç»ƒå™¨é…ç½®

```python
from neuralop import Trainer

trainer = Trainer(
    model=model,
    n_epochs=300,                    # è®­ç»ƒè½®æ•°
    device=device,                   # CPU æˆ– GPU
    data_processor=data_processor,   # æ•°æ®é¢„å¤„ç†å™¨
    mixed_precision=False,           # æ··åˆç²¾åº¦è®­ç»ƒ
    wandb_log=False,                 # æ˜¯å¦ä½¿ç”¨ WandB
    eval_interval=5,                 # æ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    log_output=False,                # æ˜¯å¦è®°å½•è¾“å‡º
    use_distributed=False,           # åˆ†å¸ƒå¼è®­ç»ƒ
    verbose=True,                    # è¯¦ç»†è¾“å‡º
)
```

### 6.5 å¼€å§‹è®­ç»ƒ

```python
trainer.train(
    train_loader=train_loader,
    test_loaders=test_loaders,       # å­—å…¸: {resolution: DataLoader}
    optimizer=optimizer,
    scheduler=scheduler,
    regularizer=False,               # æ˜¯å¦ä½¿ç”¨æ­£åˆ™åŒ–
    training_loss=train_loss,
    eval_losses=eval_losses,
)
```

### 6.6 ä½¿ç”¨è®­ç»ƒè„šæœ¬

é¡¹ç›®æä¾›äº†ç°æˆçš„è®­ç»ƒè„šæœ¬ï¼š

```bash
# è®­ç»ƒ Darcy Flow
python scripts/train_darcy.py --config config/darcy_config.py

# å¯ä»¥è¦†ç›–é…ç½®å‚æ•°
python scripts/train_darcy.py \
    --config config/darcy_config.py \
    --opt.n_epochs 500 \
    --opt.learning_rate 1e-3 \
    --data.batch_size 16
```

---

## 7. è¯„ä¼°ä¸å¯è§†åŒ–

### 7.1 æ¨¡å‹è¯„ä¼°

```python
model.eval()
with torch.no_grad():
    for data in test_loader:
        x = data['x'].to(device)
        y = data['y'].to(device)
        out = model(x)
        error = l2loss(out, y).item()
        # å¤„ç†è¯¯å·®...
```

### 7.2 å¯è§†åŒ–é¢„æµ‹ç»“æœ

```python
import matplotlib.pyplot as plt
import numpy as np

# è·å–ä¸€ä¸ªæµ‹è¯•æ ·æœ¬
data = test_samples[0]
x = data['x'].to(device)
y = data['y'].to(device)

# é¢„æµ‹
with torch.no_grad():
    out = model(x.unsqueeze(0))

# è½¬æ¢ä¸º numpy
x_np = x[0].cpu().numpy()
y_np = y.squeeze().cpu().numpy()
out_np = out.squeeze().cpu().numpy()

# ç»˜åˆ¶
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes[0].imshow(x_np, cmap='viridis')
axes[0].set_title('è¾“å…¥')
axes[1].imshow(y_np, cmap='coolwarm')
axes[1].set_title('çœŸå®è¾“å‡º')
axes[2].imshow(out_np, cmap='coolwarm')
axes[2].set_title('æ¨¡å‹é¢„æµ‹')
plt.show()
```

### 7.3 é›¶æ ·æœ¬è¶…åˆ†è¾¨ç‡

FNO çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å¯ä»¥ç›´æ¥åœ¨ä¸åŒåˆ†è¾¨ç‡ä¸Šæ¨ç†ï¼š

```python
# åœ¨ 16x16 ä¸Šè®­ç»ƒ
train_loader, _, _ = load_darcy_flow_small(
    train_resolution=16, ...
)

# åœ¨ 32x32 ä¸Šæµ‹è¯•ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒï¼ï¼‰
test_loader_32, _, _ = load_darcy_flow_small(
    test_resolutions=[32], ...
)

# ç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
model.eval()
with torch.no_grad():
    for data in test_loader_32:
        out = model(data['x'].to(device))
        # è¯„ä¼°...
```

---

## 8. å®éªŒè·Ÿè¸ª

### 8.1 Weights & Biases (WandB) è®¾ç½®

1. **è·å– API Key**:
   ```bash
   # ç™»å½• WandB
   wandb login
   # æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
   export WANDB_API_KEY="your_api_key"
   ```

2. **åœ¨ä»£ç ä¸­å¯ç”¨**:
   ```python
   import wandb
   
   wandb.init(
       project="fno-experiments",
       name="darcy-fno-small",
       config=config_dict
   )
   
   # åœ¨è®­ç»ƒå™¨ä¸­å¯ç”¨
   trainer = Trainer(
       ...,
       wandb_log=True,
   )
   ```

3. **é…ç½®æ–‡ä»¶è®¾ç½®**:
   ```python
   # config/wandb.py
   class WandbConfig(ConfigBase):
       log: bool = True
       project: str = "fno-experiments"
       entity: str = "your-entity"
       name: str = None  # è‡ªåŠ¨ç”Ÿæˆ
   ```

### 8.2 ä¿å­˜å’ŒåŠ è½½æ£€æŸ¥ç‚¹

```python
# ä¿å­˜
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'epoch': epoch,
    'loss': loss,
}, 'checkpoint.pth')

# åŠ è½½
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
```

---

## 9. å¸¸è§é—®é¢˜æ’æŸ¥

### 9.1 CUDA ç›¸å…³é”™è¯¯

**é—®é¢˜**: `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
- å‡å° `batch_size`
- å‡å° `hidden_channels` æˆ– `n_modes`
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ: `mixed_precision=True`
- ä½¿ç”¨å¼ é‡åˆ†è§£: `factorization="Tucker", rank=0.1`
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

**é—®é¢˜**: `CUDA not available`

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ PyTorch æ˜¯å¦æ­£ç¡®å®‰è£… CUDA ç‰ˆæœ¬
- éªŒè¯ GPU é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬å…¼å®¹æ€§
- å¦‚æœåªæœ‰ CPUï¼Œè®¾ç½® `device=torch.device("cpu")`

### 9.2 æ•°æ®åŠ è½½é”™è¯¯

**é—®é¢˜**: æ•°æ®ä¸‹è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- æ‰‹åŠ¨ä¸‹è½½æ•°æ®åˆ°æŒ‡å®šç›®å½•
- æ£€æŸ¥æ•°æ®è·¯å¾„æƒé™

**é—®é¢˜**: æ•°æ®å½¢çŠ¶ä¸åŒ¹é…

**è§£å†³æ–¹æ¡ˆ**:
- ç¡®è®¤è¾“å…¥/è¾“å‡ºé€šé“æ•°åŒ¹é… `in_channels` å’Œ `out_channels`
- æ£€æŸ¥æ•°æ®ç»´åº¦é¡ºåº: `[batch, channels, height, width]`

### 9.3 è®­ç»ƒä¸æ”¶æ•›

**é—®é¢˜**: æŸå¤±ä¸ä¸‹é™

**è§£å†³æ–¹æ¡ˆ**:
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå°è¯• 1e-4 åˆ° 1e-2ï¼‰
- æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–æ˜¯å¦æ­£ç¡®
- å¢åŠ æ¨¡å‹å®¹é‡ï¼ˆ`hidden_channels`, `n_layers`ï¼‰
- ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ï¼ˆå°è¯• H1Lossï¼‰
- æ£€æŸ¥æ•°æ®è´¨é‡

**é—®é¢˜**: è®­ç»ƒè¿‡æ…¢

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ GPU è€Œé CPU
- å‡å° `batch_size` å¦‚æœå—å†…å­˜é™åˆ¶
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- å‡å°‘ `n_modes` æˆ– `hidden_channels`

### 9.4 é…ç½®é”™è¯¯

**é—®é¢˜**: `n_modes` å¤ªå¤§

**è§£å†³æ–¹æ¡ˆ**:
- `n_modes` å¿…é¡»å°äº `max_resolution // 2`ï¼ˆNyquist é¢‘ç‡ï¼‰
- å¯¹äº 16x16 åˆ†è¾¨ç‡ï¼Œ`n_modes` åº” â‰¤ 8
- å¯¹äº 32x32 åˆ†è¾¨ç‡ï¼Œ`n_modes` åº” â‰¤ 16

---

## 10. å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

### 10.1 æœ€å°ç¤ºä¾‹

```python
import torch
from neuralop.models import FNO
from neuralop import Trainer, H1Loss, LpLoss
from neuralop.training import AdamW
from neuralop.data.datasets import load_darcy_flow_small

# 1. è®¾å¤‡è®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. åŠ è½½æ•°æ®
train_loader, test_loaders, data_processor = load_darcy_flow_small(
    n_train=1000,
    batch_size=32,
    n_tests=[100, 50],
    test_resolutions=[16, 32],
    test_batch_sizes=[32, 32],
)
data_processor = data_processor.to(device)

# 3. åˆ›å»ºæ¨¡å‹
model = FNO(
    n_modes=(8, 8),
    in_channels=1,
    out_channels=1,
    hidden_channels=32,
    n_layers=4,
).to(device)

# 4. è®¾ç½®è®­ç»ƒç»„ä»¶
optimizer = AdamW(model.parameters(), lr=8e-3, weight_decay=1e-4)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
train_loss = H1Loss(d=2)
eval_losses = {"h1": train_loss, "l2": LpLoss(d=2, p=2)}

# 5. è®­ç»ƒ
trainer = Trainer(
    model=model,
    n_epochs=20,
    device=device,
    data_processor=data_processor,
    wandb_log=False,
    verbose=True,
)
trainer.train(
    train_loader=train_loader,
    test_loaders=test_loaders,
    optimizer=optimizer,
    scheduler=scheduler,
    training_loss=train_loss,
    eval_losses=eval_losses,
)
```

### 10.2 ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from zencfg import make_config_from_cli
from config.darcy_config import Default
from neuralop import get_model, Trainer
from neuralop.training import AdamW
from neuralop.data.datasets import load_darcy_flow_small

# åŠ è½½é…ç½®
config = make_config_from_cli(Default)
config = config.to_dict()

# åŠ è½½æ•°æ®å’Œæ¨¡å‹
train_loader, test_loaders, data_processor = load_darcy_flow_small(...)
model = get_model(config)

# è®¾ç½®è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
# ... (å‚è€ƒ scripts/train_darcy.py)
```

### 10.3 è¿è¡Œå®Œæ•´ç¤ºä¾‹

é¡¹ç›®æ ¹ç›®å½•æä¾›äº†å®Œæ•´ç¤ºä¾‹ï¼š

```bash
# è¿è¡Œå®Œæ•´ç¤ºä¾‹ï¼ˆåŒ…å«å¯è§†åŒ–ï¼‰
python complete_example.py

# æˆ–è¿è¡Œç®€å•ç¤ºä¾‹
python simple_complete_example.py
```

---

## ğŸ“š å‚è€ƒèµ„æº

### æ–‡æ¡£
- é¡¹ç›® README: `README.rst`
- API æ–‡æ¡£: `doc/source/`
- ç¤ºä¾‹ä»£ç : `examples/`

### é…ç½®æ–‡ä»¶
- æ¨¡å‹é…ç½®: `config/models.py`
- Darcy é…ç½®: `config/darcy_config.py`
- ä¼˜åŒ–é…ç½®: `config/opt.py`

### è®­ç»ƒè„šæœ¬
- Darcy Flow: `scripts/train_darcy.py`
- å…¶ä»– PDE: `scripts/train_*.py`

### ç¤ºä¾‹
- FNO Darcy: `examples/models/plot_FNO_darcy.py`
- å®Œæ•´ç¤ºä¾‹: `complete_example.py`

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹å®éªŒå‰ï¼Œç¡®è®¤ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] Python ç¯å¢ƒå·²è®¾ç½®ï¼ˆ3.8+ï¼‰
- [ ] è™šæ‹Ÿç¯å¢ƒå·²åˆ›å»ºå¹¶æ¿€æ´»
- [ ] æ‰€æœ‰ä¾èµ–å·²å®‰è£…
- [ ] NeuralOperator åŒ…å·²å®‰è£…
- [ ] GPU å¯ç”¨ï¼ˆå¦‚éœ€è¦ï¼‰æˆ– CPU é…ç½®æ­£ç¡®
- [ ] æ•°æ®å·²å‡†å¤‡æˆ–ä¸‹è½½è·¯å¾„å·²é…ç½®
- [ ] æ¨¡å‹é…ç½®å·²è®¾ç½®ï¼ˆ`n_modes`, `hidden_channels` ç­‰ï¼‰
- [ ] è®­ç»ƒå‚æ•°å·²é…ç½®ï¼ˆå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰ï¼‰
- [ ] WandB å·²é…ç½®ï¼ˆå¦‚ä½¿ç”¨ï¼‰
- [ ] å­˜å‚¨ç©ºé—´å……è¶³
- [ ] å·²é˜…è¯»ç›¸å…³æ–‡æ¡£å’Œç¤ºä¾‹

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **è¿è¡Œå¿«é€Ÿç¤ºä¾‹**: å…ˆè¿è¡Œ `complete_example.py` éªŒè¯ç¯å¢ƒ
2. **è°ƒæ•´é…ç½®**: æ ¹æ®ä½ çš„é—®é¢˜è°ƒæ•´æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
3. **å®éªŒè¿­ä»£**: å°è¯•ä¸åŒçš„è¶…å‚æ•°ç»„åˆ
4. **ç»“æœåˆ†æ**: ä½¿ç”¨å¯è§†åŒ–å·¥å…·åˆ†ææ¨¡å‹æ€§èƒ½
5. **æ‰©å±•åˆ°æ–°é—®é¢˜**: å°† FNO åº”ç”¨åˆ°ä½ çš„å…·ä½“é—®é¢˜

---

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒé¡¹ç›®æ–‡æ¡£æˆ–æäº¤ Issueã€‚

